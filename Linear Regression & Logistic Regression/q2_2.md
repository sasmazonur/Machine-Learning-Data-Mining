Batch gradient descent to train a binary logistic regression classifier using L2 regularization:

Load test and train data sets.
Normalize.
Batch loop:
	Loop over training examples:
		Calculate gradient vector of objective function with respect to weights:
			<img src="https://render.githubusercontent.com/render/math?math=\nabla L=\frac{1}{m}(X)^T\times (g(X\textbf{w})-y)+\frac{1}{m}\lambda \textbf{w}">
	Update weight vector:
		<img src="https://render.githubusercontent.com/render/math?math=\textbf{w} \leftarrow \textbf{w}-\alpha\nabla L">
		
<img src="https://render.githubusercontent.com/render/math?math=\alpha, \lambda"> can be manipulated to optimize the model's accuracy and the training time required. A learning rate of 0.1 and regularization parameter <img src="https://render.githubusercontent.com/render/math?math=10^{-3}\leq\lambda\leq10^3"> give accuracies of around 90 % after 10 iterations.
